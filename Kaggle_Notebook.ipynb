{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55bb2e28",
   "metadata": {
    "papermill": {
     "duration": 0.002817,
     "end_time": "2025-01-07T21:39:44.895697",
     "exception": false,
     "start_time": "2025-01-07T21:39:44.892880",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Some cells to run code (you may activate GPUs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "395a1873",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T21:39:44.902884Z",
     "iopub.status.busy": "2025-01-07T21:39:44.902506Z",
     "iopub.status.idle": "2025-01-07T21:39:47.328973Z",
     "shell.execute_reply": "2025-01-07T21:39:47.327293Z"
    },
    "papermill": {
     "duration": 2.431969,
     "end_time": "2025-01-07T21:39:47.330970",
     "exception": false,
     "start_time": "2025-01-07T21:39:44.899001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ALTEGRAD_Kaggle'...\r\n",
      "remote: Enumerating objects: 16906, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (16906/16906), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (7258/7258), done.\u001b[K\r\n",
      "remote: Total 16906 (delta 9643), reused 16904 (delta 9641), pack-reused 0 (from 0)\u001b[K\r\n",
      "Receiving objects: 100% (16906/16906), 5.51 MiB | 16.61 MiB/s, done.\r\n",
      "Resolving deltas: 100% (9643/9643), done.\r\n",
      "/kaggle/working/ALTEGRAD_Kaggle\n"
     ]
    }
   ],
   "source": [
    "# To import code and data\n",
    "!git clone https://github.com/SoelMgd/ALTEGRAD_Kaggle.git\n",
    "%cd ALTEGRAD_Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "923997b9",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-07T21:39:47.339368Z",
     "iopub.status.busy": "2025-01-07T21:39:47.338959Z",
     "iopub.status.idle": "2025-01-07T21:40:10.857761Z",
     "shell.execute_reply": "2025-01-07T21:40:10.855943Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 23.52559,
     "end_time": "2025-01-07T21:40:10.860107",
     "exception": false,
     "start_time": "2025-01-07T21:39:47.334517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting grakel\r\n",
      "  Downloading GraKeL-0.1.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\r\n",
      "Collecting torch_geometric\r\n",
      "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting scipy==1.7.3\r\n",
      "  Downloading scipy-1.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\r\n",
      "Collecting numpy<1.23.0,>=1.16.5 (from scipy==1.7.3)\r\n",
      "  Downloading numpy-1.22.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\r\n",
      "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.10/dist-packages (from grakel) (3.0.11)\r\n",
      "Requirement already satisfied: scikit-learn>=0.19 in /usr/local/lib/python3.10/dist-packages (from grakel) (1.2.2)\r\n",
      "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from grakel) (1.16.0)\r\n",
      "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from grakel) (1.0.0)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from grakel) (1.4.2)\r\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.10.5)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.6.1)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\r\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\r\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.5)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19->grakel) (3.5.0)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.11.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.8.30)\r\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\r\n",
      "Downloading scipy-1.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading GraKeL-0.1.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading numpy-1.22.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: numpy, scipy, torch_geometric, grakel\r\n",
      "  Attempting uninstall: numpy\r\n",
      "    Found existing installation: numpy 1.26.4\r\n",
      "    Uninstalling numpy-1.26.4:\r\n",
      "      Successfully uninstalled numpy-1.26.4\r\n",
      "  Attempting uninstall: scipy\r\n",
      "    Found existing installation: scipy 1.13.1\r\n",
      "    Uninstalling scipy-1.13.1:\r\n",
      "      Successfully uninstalled scipy-1.13.1\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cartopy 0.24.1 requires numpy>=1.23, but you have numpy 1.22.4 which is incompatible.\r\n",
      "albucore 0.0.16 requires numpy>=1.24, but you have numpy 1.22.4 which is incompatible.\r\n",
      "albumentations 1.4.15 requires numpy>=1.24.4, but you have numpy 1.22.4 which is incompatible.\r\n",
      "albumentations 1.4.15 requires scipy>=1.10.0, but you have scipy 1.7.3 which is incompatible.\r\n",
      "arviz 0.19.0 requires numpy>=1.23.0, but you have numpy 1.22.4 which is incompatible.\r\n",
      "arviz 0.19.0 requires scipy>=1.9.0, but you have scipy 1.7.3 which is incompatible.\r\n",
      "astropy 6.1.3 requires numpy>=1.23, but you have numpy 1.22.4 which is incompatible.\r\n",
      "bayesian-optimization 2.0.1 requires numpy>=1.25, but you have numpy 1.22.4 which is incompatible.\r\n",
      "bigframes 1.17.0 requires numpy>=1.24.0, but you have numpy 1.22.4 which is incompatible.\r\n",
      "chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.22.4 which is incompatible.\r\n",
      "contourpy 1.3.0 requires numpy>=1.23, but you have numpy 1.22.4 which is incompatible.\r\n",
      "cudf-cu12 24.4.1 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.4 which is incompatible.\r\n",
      "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 18.1.0 which is incompatible.\r\n",
      "dipy 1.10.0 requires scipy>=1.8, but you have scipy 1.7.3 which is incompatible.\r\n",
      "featuretools 1.31.0 requires numpy>=1.25.0, but you have numpy 1.22.4 which is incompatible.\r\n",
      "featuretools 1.31.0 requires scipy>=1.10.0, but you have scipy 1.7.3 which is incompatible.\r\n",
      "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 18.1.0 which is incompatible.\r\n",
      "jax 0.4.26 requires scipy>=1.9, but you have scipy 1.7.3 which is incompatible.\r\n",
      "jaxlib 0.4.26+cuda12.cudnn89 requires scipy>=1.9, but you have scipy 1.7.3 which is incompatible.\r\n",
      "kaggle-environments 1.16.10 requires scipy>=1.11.2, but you have scipy 1.7.3 which is incompatible.\r\n",
      "libpysal 4.9.2 requires scipy>=1.8, but you have scipy 1.7.3 which is incompatible.\r\n",
      "mizani 0.11.4 requires numpy>=1.23.0, but you have numpy 1.22.4 which is incompatible.\r\n",
      "mne 1.8.0 requires numpy<3,>=1.23, but you have numpy 1.22.4 which is incompatible.\r\n",
      "mne 1.8.0 requires scipy>=1.9, but you have scipy 1.7.3 which is incompatible.\r\n",
      "nilearn 0.10.4 requires scipy>=1.8.0, but you have scipy 1.7.3 which is incompatible.\r\n",
      "numexpr 2.10.1 requires numpy>=1.23.0, but you have numpy 1.22.4 which is incompatible.\r\n",
      "pandas-gbq 0.23.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "pandas-stubs 2.1.4.231227 requires numpy>=1.26.0; python_version < \"3.13\", but you have numpy 1.22.4 which is incompatible.\r\n",
      "plotnine 0.13.6 requires numpy>=1.23.0, but you have numpy 1.22.4 which is incompatible.\r\n",
      "pyldavis 3.4.1 requires numpy>=1.24.2, but you have numpy 1.22.4 which is incompatible.\r\n",
      "pywavelets 1.8.0 requires numpy<3,>=1.23, but you have numpy 1.22.4 which is incompatible.\r\n",
      "rmm-cu12 24.4.0 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.4 which is incompatible.\r\n",
      "scikit-image 0.24.0 requires numpy>=1.23, but you have numpy 1.22.4 which is incompatible.\r\n",
      "scikit-image 0.24.0 requires scipy>=1.9, but you have scipy 1.7.3 which is incompatible.\r\n",
      "statsmodels 0.14.3 requires scipy!=1.9.2,>=1.8, but you have scipy 1.7.3 which is incompatible.\r\n",
      "stumpy 1.13.0 requires scipy>=1.10, but you have scipy 1.7.3 which is incompatible.\r\n",
      "tensorflow 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 1.22.4 which is incompatible.\r\n",
      "visions 0.7.6 requires numpy>=1.23.2, but you have numpy 1.22.4 which is incompatible.\r\n",
      "woodwork 0.31.0 requires numpy>=1.25.0, but you have numpy 1.22.4 which is incompatible.\r\n",
      "woodwork 0.31.0 requires scipy>=1.10.0, but you have scipy 1.7.3 which is incompatible.\r\n",
      "xarray 2024.9.0 requires numpy>=1.24, but you have numpy 1.22.4 which is incompatible.\r\n",
      "xarray-einstats 0.8.0 requires numpy>=1.23, but you have numpy 1.22.4 which is incompatible.\r\n",
      "xarray-einstats 0.8.0 requires scipy>=1.9, but you have scipy 1.7.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed grakel-0.1.10 numpy-1.22.4 scipy-1.7.3 torch_geometric-2.6.1\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# To install requirements\n",
    "%pip install grakel torch_geometric scipy==1.7.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0add3d26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T21:40:10.873985Z",
     "iopub.status.busy": "2025-01-07T21:40:10.873613Z",
     "iopub.status.idle": "2025-01-07T21:40:11.426998Z",
     "shell.execute_reply": "2025-01-07T21:40:11.425138Z"
    },
    "papermill": {
     "duration": 0.562866,
     "end_time": "2025-01-07T21:40:11.429550",
     "exception": false,
     "start_time": "2025-01-07T21:40:10.866684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From https://github.com/SoelMgd/ALTEGRAD_Kaggle\r\n",
      " * branch              main       -> FETCH_HEAD\r\n",
      "Already up to date.\r\n"
     ]
    }
   ],
   "source": [
    "# If you made changes in the code\n",
    "!git pull origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cd0ff62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T21:40:11.444199Z",
     "iopub.status.busy": "2025-01-07T21:40:11.443793Z",
     "iopub.status.idle": "2025-01-08T03:38:23.512152Z",
     "shell.execute_reply": "2025-01-08T03:38:23.510791Z"
    },
    "papermill": {
     "duration": 21492.079754,
     "end_time": "2025-01-08T03:38:23.515471",
     "exception": false,
     "start_time": "2025-01-07T21:40:11.435717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 8000/8000 [00:49<00:00, 163.04it/s]\r\n",
      "Dataset ./data/dataset_train.pt saved\r\n",
      "100%|██████████████████████████████████████| 1000/1000 [00:05<00:00, 173.73it/s]\r\n",
      "Dataset ./data/dataset_valid.pt saved\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 64, 'latent_dim': 16, 'lr': 0.001, 'n_layers_decoder': 3, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 64, 'latent_dim': 16, 'lr': 0.001, 'n_layers_decoder': 3, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 64, 'latent_dim': 16, 'lr': 0.001, 'n_layers_decoder': 4, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 64, 'latent_dim': 16, 'lr': 0.001, 'n_layers_decoder': 4, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 64, 'latent_dim': 16, 'lr': 0.01, 'n_layers_decoder': 3, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 64, 'latent_dim': 16, 'lr': 0.01, 'n_layers_decoder': 3, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 64, 'latent_dim': 16, 'lr': 0.01, 'n_layers_decoder': 4, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 64, 'latent_dim': 16, 'lr': 0.01, 'n_layers_decoder': 4, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 64, 'latent_dim': 32, 'lr': 0.001, 'n_layers_decoder': 3, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 64, 'latent_dim': 32, 'lr': 0.001, 'n_layers_decoder': 3, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 64, 'latent_dim': 32, 'lr': 0.001, 'n_layers_decoder': 4, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 64, 'latent_dim': 32, 'lr': 0.001, 'n_layers_decoder': 4, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 64, 'latent_dim': 32, 'lr': 0.01, 'n_layers_decoder': 3, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 64, 'latent_dim': 32, 'lr': 0.01, 'n_layers_decoder': 3, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 64, 'latent_dim': 32, 'lr': 0.01, 'n_layers_decoder': 4, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 64, 'latent_dim': 32, 'lr': 0.01, 'n_layers_decoder': 4, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 128, 'latent_dim': 16, 'lr': 0.001, 'n_layers_decoder': 3, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 128, 'latent_dim': 16, 'lr': 0.001, 'n_layers_decoder': 3, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 128, 'latent_dim': 16, 'lr': 0.001, 'n_layers_decoder': 4, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 128, 'latent_dim': 16, 'lr': 0.001, 'n_layers_decoder': 4, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 128, 'latent_dim': 16, 'lr': 0.01, 'n_layers_decoder': 3, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 128, 'latent_dim': 16, 'lr': 0.01, 'n_layers_decoder': 3, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 128, 'latent_dim': 16, 'lr': 0.01, 'n_layers_decoder': 4, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 128, 'latent_dim': 16, 'lr': 0.01, 'n_layers_decoder': 4, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 128, 'latent_dim': 32, 'lr': 0.001, 'n_layers_decoder': 3, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 128, 'latent_dim': 32, 'lr': 0.001, 'n_layers_decoder': 3, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 128, 'latent_dim': 32, 'lr': 0.001, 'n_layers_decoder': 4, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 128, 'latent_dim': 32, 'lr': 0.001, 'n_layers_decoder': 4, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 128, 'latent_dim': 32, 'lr': 0.01, 'n_layers_decoder': 3, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 128, 'latent_dim': 32, 'lr': 0.01, 'n_layers_decoder': 3, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 128, 'latent_dim': 32, 'lr': 0.01, 'n_layers_decoder': 4, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 128, 'latent_dim': 32, 'lr': 0.01, 'n_layers_decoder': 4, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 64, 'latent_dim': 16, 'lr': 0.001, 'n_layers_decoder': 3, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 64, 'latent_dim': 16, 'lr': 0.001, 'n_layers_decoder': 3, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 64, 'latent_dim': 16, 'lr': 0.001, 'n_layers_decoder': 4, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 64, 'latent_dim': 16, 'lr': 0.001, 'n_layers_decoder': 4, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 64, 'latent_dim': 16, 'lr': 0.01, 'n_layers_decoder': 3, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 64, 'latent_dim': 16, 'lr': 0.01, 'n_layers_decoder': 3, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 64, 'latent_dim': 16, 'lr': 0.01, 'n_layers_decoder': 4, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 64, 'latent_dim': 16, 'lr': 0.01, 'n_layers_decoder': 4, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 64, 'latent_dim': 32, 'lr': 0.001, 'n_layers_decoder': 3, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 64, 'latent_dim': 32, 'lr': 0.001, 'n_layers_decoder': 3, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 64, 'latent_dim': 32, 'lr': 0.001, 'n_layers_decoder': 4, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 64, 'latent_dim': 32, 'lr': 0.001, 'n_layers_decoder': 4, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 64, 'latent_dim': 32, 'lr': 0.01, 'n_layers_decoder': 3, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 64, 'latent_dim': 32, 'lr': 0.01, 'n_layers_decoder': 3, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 64, 'latent_dim': 32, 'lr': 0.01, 'n_layers_decoder': 4, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 64, 'latent_dim': 32, 'lr': 0.01, 'n_layers_decoder': 4, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 128, 'latent_dim': 16, 'lr': 0.001, 'n_layers_decoder': 3, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 128, 'latent_dim': 16, 'lr': 0.001, 'n_layers_decoder': 3, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 128, 'latent_dim': 16, 'lr': 0.001, 'n_layers_decoder': 4, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 128, 'latent_dim': 16, 'lr': 0.001, 'n_layers_decoder': 4, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 128, 'latent_dim': 16, 'lr': 0.01, 'n_layers_decoder': 3, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 128, 'latent_dim': 16, 'lr': 0.01, 'n_layers_decoder': 3, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 128, 'latent_dim': 16, 'lr': 0.01, 'n_layers_decoder': 4, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 128, 'latent_dim': 16, 'lr': 0.01, 'n_layers_decoder': 4, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 128, 'latent_dim': 32, 'lr': 0.001, 'n_layers_decoder': 3, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 128, 'latent_dim': 32, 'lr': 0.001, 'n_layers_decoder': 3, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 128, 'latent_dim': 32, 'lr': 0.001, 'n_layers_decoder': 4, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 128, 'latent_dim': 32, 'lr': 0.001, 'n_layers_decoder': 4, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 128, 'latent_dim': 32, 'lr': 0.01, 'n_layers_decoder': 3, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 128, 'latent_dim': 32, 'lr': 0.01, 'n_layers_decoder': 3, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 128, 'latent_dim': 32, 'lr': 0.01, 'n_layers_decoder': 4, 'n_layers_encoder': 2, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Testing parameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 512, 'hidden_dim_encoder': 128, 'latent_dim': 32, 'lr': 0.01, 'n_layers_decoder': 4, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Best hyperparameters: {'epochs_autoencoder': 50, 'hidden_dim_decoder': 256, 'hidden_dim_encoder': 128, 'latent_dim': 32, 'lr': 0.01, 'n_layers_decoder': 3, 'n_layers_encoder': 3, 'n_max_nodes': 50, 'spectral_emb_dim': 10}\r\n",
      "Best validation loss: 0.20411375164985657\r\n"
     ]
    }
   ],
   "source": [
    "# Do cross-validation\n",
    "!python code/cross_validation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "136b34dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T03:38:23.590759Z",
     "iopub.status.busy": "2025-01-08T03:38:23.590325Z",
     "iopub.status.idle": "2025-01-08T05:13:55.180297Z",
     "shell.execute_reply": "2025-01-08T05:13:55.178989Z"
    },
    "papermill": {
     "duration": 5731.630317,
     "end_time": "2025-01-08T05:13:55.182563",
     "exception": false,
     "start_time": "2025-01-08T03:38:23.552246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/ALTEGRAD_Kaggle/code/utils.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  data_lst = torch.load(filename)\r\n",
      "Dataset ./data/dataset_train.pt loaded from file\r\n",
      "Dataset ./data/dataset_valid.pt loaded from file\r\n",
      "Dataset ./data/dataset_test.pt saved\r\n",
      "08/01/2025 03:38:51 Epoch: 0001, Train Loss: 820.44823, Train Reconstruction Loss: 0.22, Train KLD Loss: 16404.53, Val Loss: 85.82877, Val Reconstruction Loss: 0.28, Val KLD Loss: 1711.02\r\n",
      "08/01/2025 03:39:11 Epoch: 0002, Train Loss: 17.81901, Train Reconstruction Loss: 0.22, Train KLD Loss: 352.06, Val Loss: 7.15505, Val Reconstruction Loss: 0.27, Val KLD Loss: 137.67\r\n",
      "08/01/2025 03:39:31 Epoch: 0003, Train Loss: 5.03831, Train Reconstruction Loss: 0.22, Train KLD Loss: 96.43, Val Loss: 2.03969, Val Reconstruction Loss: 0.26, Val KLD Loss: 35.51\r\n",
      "08/01/2025 03:39:50 Epoch: 0004, Train Loss: 2.63602, Train Reconstruction Loss: 0.22, Train KLD Loss: 48.33, Val Loss: 1.40380, Val Reconstruction Loss: 0.25, Val KLD Loss: 23.17\r\n",
      "08/01/2025 03:40:10 Epoch: 0005, Train Loss: 2.59766, Train Reconstruction Loss: 0.22, Train KLD Loss: 47.60, Val Loss: 1.57667, Val Reconstruction Loss: 0.24, Val KLD Loss: 26.71\r\n",
      "08/01/2025 03:40:29 Epoch: 0006, Train Loss: 2.10504, Train Reconstruction Loss: 0.22, Train KLD Loss: 37.75, Val Loss: 0.95914, Val Reconstruction Loss: 0.23, Val KLD Loss: 14.64\r\n",
      "08/01/2025 03:40:48 Epoch: 0007, Train Loss: 1.88921, Train Reconstruction Loss: 0.22, Train KLD Loss: 33.43, Val Loss: 1.20352, Val Reconstruction Loss: 0.23, Val KLD Loss: 19.52\r\n",
      "08/01/2025 03:41:08 Epoch: 0008, Train Loss: 1.72795, Train Reconstruction Loss: 0.22, Train KLD Loss: 30.18, Val Loss: 1.11769, Val Reconstruction Loss: 0.22, Val KLD Loss: 17.96\r\n",
      "08/01/2025 03:41:27 Epoch: 0009, Train Loss: 2.50451, Train Reconstruction Loss: 0.22, Train KLD Loss: 45.72, Val Loss: 1.70948, Val Reconstruction Loss: 0.22, Val KLD Loss: 29.77\r\n",
      "08/01/2025 03:41:47 Epoch: 0010, Train Loss: 1.50565, Train Reconstruction Loss: 0.22, Train KLD Loss: 25.76, Val Loss: 0.79387, Val Reconstruction Loss: 0.21, Val KLD Loss: 11.63\r\n",
      "08/01/2025 03:42:05 Epoch: 0011, Train Loss: 1.26364, Train Reconstruction Loss: 0.22, Train KLD Loss: 20.90, Val Loss: 0.56384, Val Reconstruction Loss: 0.21, Val KLD Loss: 7.05\r\n",
      "08/01/2025 03:42:25 Epoch: 0012, Train Loss: 1.12261, Train Reconstruction Loss: 0.22, Train KLD Loss: 18.10, Val Loss: 0.45516, Val Reconstruction Loss: 0.21, Val KLD Loss: 4.90\r\n",
      "08/01/2025 03:42:45 Epoch: 0013, Train Loss: 1.02729, Train Reconstruction Loss: 0.22, Train KLD Loss: 16.17, Val Loss: 0.45057, Val Reconstruction Loss: 0.21, Val KLD Loss: 4.87\r\n",
      "08/01/2025 03:43:04 Epoch: 0014, Train Loss: 2.57300, Train Reconstruction Loss: 0.22, Train KLD Loss: 47.11, Val Loss: 15.13523, Val Reconstruction Loss: 0.21, Val KLD Loss: 298.47\r\n",
      "08/01/2025 03:43:24 Epoch: 0015, Train Loss: 22.47689, Train Reconstruction Loss: 0.22, Train KLD Loss: 445.16, Val Loss: 9.71983, Val Reconstruction Loss: 0.22, Val KLD Loss: 190.05\r\n",
      "08/01/2025 03:43:44 Epoch: 0016, Train Loss: 16.08129, Train Reconstruction Loss: 0.22, Train KLD Loss: 317.22, Val Loss: 25.60920, Val Reconstruction Loss: 0.21, Val KLD Loss: 507.92\r\n",
      "08/01/2025 03:44:03 Epoch: 0017, Train Loss: 12.25457, Train Reconstruction Loss: 0.22, Train KLD Loss: 240.74, Val Loss: 1.23577, Val Reconstruction Loss: 0.21, Val KLD Loss: 20.44\r\n",
      "08/01/2025 03:44:23 Epoch: 0018, Train Loss: 1.78361, Train Reconstruction Loss: 0.22, Train KLD Loss: 31.30, Val Loss: 0.50255, Val Reconstruction Loss: 0.21, Val KLD Loss: 5.86\r\n",
      "08/01/2025 03:44:42 Epoch: 0019, Train Loss: 1.70616, Train Reconstruction Loss: 0.22, Train KLD Loss: 29.77, Val Loss: 0.45926, Val Reconstruction Loss: 0.21, Val KLD Loss: 4.93\r\n",
      "08/01/2025 03:45:02 Epoch: 0020, Train Loss: 1.02780, Train Reconstruction Loss: 0.22, Train KLD Loss: 16.13, Val Loss: 0.38150, Val Reconstruction Loss: 0.21, Val KLD Loss: 3.46\r\n",
      "08/01/2025 03:45:22 Epoch: 0021, Train Loss: 0.83315, Train Reconstruction Loss: 0.22, Train KLD Loss: 12.29, Val Loss: 0.31964, Val Reconstruction Loss: 0.21, Val KLD Loss: 2.21\r\n",
      "08/01/2025 03:45:41 Epoch: 0022, Train Loss: 0.82653, Train Reconstruction Loss: 0.22, Train KLD Loss: 12.16, Val Loss: 0.29613, Val Reconstruction Loss: 0.20, Val KLD Loss: 1.85\r\n",
      "08/01/2025 03:46:00 Epoch: 0023, Train Loss: 0.79354, Train Reconstruction Loss: 0.22, Train KLD Loss: 11.49, Val Loss: 0.37859, Val Reconstruction Loss: 0.21, Val KLD Loss: 3.35\r\n",
      "08/01/2025 03:46:19 Epoch: 0024, Train Loss: 0.75947, Train Reconstruction Loss: 0.22, Train KLD Loss: 10.77, Val Loss: 0.30672, Val Reconstruction Loss: 0.21, Val KLD Loss: 1.96\r\n",
      "08/01/2025 03:46:40 Epoch: 0025, Train Loss: 0.74566, Train Reconstruction Loss: 0.22, Train KLD Loss: 10.54, Val Loss: 0.36555, Val Reconstruction Loss: 0.21, Val KLD Loss: 3.14\r\n",
      "08/01/2025 03:46:59 Epoch: 0026, Train Loss: 0.73411, Train Reconstruction Loss: 0.22, Train KLD Loss: 10.31, Val Loss: 0.32729, Val Reconstruction Loss: 0.20, Val KLD Loss: 2.45\r\n",
      "08/01/2025 03:47:18 Epoch: 0027, Train Loss: 0.69668, Train Reconstruction Loss: 0.22, Train KLD Loss: 9.58, Val Loss: 0.33618, Val Reconstruction Loss: 0.21, Val KLD Loss: 2.59\r\n",
      "08/01/2025 03:47:38 Epoch: 0028, Train Loss: 0.69885, Train Reconstruction Loss: 0.22, Train KLD Loss: 9.60, Val Loss: 0.38964, Val Reconstruction Loss: 0.21, Val KLD Loss: 3.65\r\n",
      "08/01/2025 03:47:57 Epoch: 0029, Train Loss: 0.66300, Train Reconstruction Loss: 0.22, Train KLD Loss: 8.88, Val Loss: 0.28583, Val Reconstruction Loss: 0.21, Val KLD Loss: 1.56\r\n",
      "08/01/2025 03:48:18 Epoch: 0030, Train Loss: 0.70086, Train Reconstruction Loss: 0.22, Train KLD Loss: 9.63, Val Loss: 0.29764, Val Reconstruction Loss: 0.21, Val KLD Loss: 1.78\r\n",
      "08/01/2025 03:48:37 Epoch: 0031, Train Loss: 0.64147, Train Reconstruction Loss: 0.22, Train KLD Loss: 8.45, Val Loss: 0.28166, Val Reconstruction Loss: 0.21, Val KLD Loss: 1.39\r\n",
      "08/01/2025 03:48:57 Epoch: 0032, Train Loss: 0.63654, Train Reconstruction Loss: 0.22, Train KLD Loss: 8.33, Val Loss: 0.30457, Val Reconstruction Loss: 0.21, Val KLD Loss: 1.89\r\n",
      "08/01/2025 03:49:17 Epoch: 0033, Train Loss: 0.61084, Train Reconstruction Loss: 0.22, Train KLD Loss: 7.85, Val Loss: 0.31754, Val Reconstruction Loss: 0.21, Val KLD Loss: 2.20\r\n",
      "08/01/2025 03:49:35 Epoch: 0034, Train Loss: 0.61908, Train Reconstruction Loss: 0.22, Train KLD Loss: 7.99, Val Loss: 0.29541, Val Reconstruction Loss: 0.21, Val KLD Loss: 1.69\r\n",
      "08/01/2025 03:49:54 Epoch: 0035, Train Loss: 0.59262, Train Reconstruction Loss: 0.22, Train KLD Loss: 7.47, Val Loss: 0.28347, Val Reconstruction Loss: 0.21, Val KLD Loss: 1.49\r\n",
      "08/01/2025 03:50:13 Epoch: 0036, Train Loss: 0.64283, Train Reconstruction Loss: 0.22, Train KLD Loss: 8.49, Val Loss: 0.38328, Val Reconstruction Loss: 0.21, Val KLD Loss: 3.50\r\n",
      "08/01/2025 03:50:33 Epoch: 0037, Train Loss: 0.59769, Train Reconstruction Loss: 0.22, Train KLD Loss: 7.58, Val Loss: 0.25719, Val Reconstruction Loss: 0.21, Val KLD Loss: 0.95\r\n",
      "08/01/2025 03:50:52 Epoch: 0038, Train Loss: 0.57059, Train Reconstruction Loss: 0.22, Train KLD Loss: 7.04, Val Loss: 0.26935, Val Reconstruction Loss: 0.21, Val KLD Loss: 1.25\r\n",
      "08/01/2025 03:51:11 Epoch: 0039, Train Loss: 0.58851, Train Reconstruction Loss: 0.22, Train KLD Loss: 7.41, Val Loss: 0.29553, Val Reconstruction Loss: 0.21, Val KLD Loss: 1.71\r\n",
      "08/01/2025 03:51:31 Epoch: 0040, Train Loss: 0.60841, Train Reconstruction Loss: 0.22, Train KLD Loss: 7.77, Val Loss: 0.26856, Val Reconstruction Loss: 0.21, Val KLD Loss: 1.22\r\n",
      "08/01/2025 03:51:51 Epoch: 0041, Train Loss: 0.60656, Train Reconstruction Loss: 0.22, Train KLD Loss: 7.76, Val Loss: 0.29370, Val Reconstruction Loss: 0.21, Val KLD Loss: 1.71\r\n",
      "08/01/2025 03:52:10 Epoch: 0042, Train Loss: 0.55851, Train Reconstruction Loss: 0.22, Train KLD Loss: 6.81, Val Loss: 0.30847, Val Reconstruction Loss: 0.21, Val KLD Loss: 1.99\r\n",
      "08/01/2025 03:52:30 Epoch: 0043, Train Loss: 0.58779, Train Reconstruction Loss: 0.22, Train KLD Loss: 7.39, Val Loss: 0.25906, Val Reconstruction Loss: 0.21, Val KLD Loss: 0.97\r\n",
      "08/01/2025 03:52:49 Epoch: 0044, Train Loss: 0.53062, Train Reconstruction Loss: 0.22, Train KLD Loss: 6.23, Val Loss: 0.29003, Val Reconstruction Loss: 0.21, Val KLD Loss: 1.59\r\n",
      "08/01/2025 03:53:09 Epoch: 0045, Train Loss: 0.53019, Train Reconstruction Loss: 0.22, Train KLD Loss: 6.24, Val Loss: 0.27312, Val Reconstruction Loss: 0.21, Val KLD Loss: 1.25\r\n",
      "08/01/2025 03:53:29 Epoch: 0046, Train Loss: 0.52187, Train Reconstruction Loss: 0.22, Train KLD Loss: 6.06, Val Loss: 0.26931, Val Reconstruction Loss: 0.21, Val KLD Loss: 1.11\r\n",
      "08/01/2025 03:53:48 Epoch: 0047, Train Loss: 0.54821, Train Reconstruction Loss: 0.22, Train KLD Loss: 6.54, Val Loss: 0.28052, Val Reconstruction Loss: 0.21, Val KLD Loss: 1.39\r\n",
      "08/01/2025 03:54:07 Epoch: 0048, Train Loss: 0.52807, Train Reconstruction Loss: 0.22, Train KLD Loss: 6.17, Val Loss: 0.25160, Val Reconstruction Loss: 0.21, Val KLD Loss: 0.83\r\n",
      "08/01/2025 03:54:27 Epoch: 0049, Train Loss: 0.50315, Train Reconstruction Loss: 0.22, Train KLD Loss: 5.69, Val Loss: 0.26669, Val Reconstruction Loss: 0.21, Val KLD Loss: 1.12\r\n",
      "08/01/2025 03:54:46 Epoch: 0050, Train Loss: 0.54741, Train Reconstruction Loss: 0.22, Train KLD Loss: 6.59, Val Loss: 0.26998, Val Reconstruction Loss: 0.21, Val KLD Loss: 1.17\r\n",
      "08/01/2025 03:55:06 Epoch: 0051, Train Loss: 0.50233, Train Reconstruction Loss: 0.22, Train KLD Loss: 5.69, Val Loss: 0.26715, Val Reconstruction Loss: 0.21, Val KLD Loss: 1.13\r\n",
      "08/01/2025 03:55:25 Epoch: 0052, Train Loss: 0.47498, Train Reconstruction Loss: 0.22, Train KLD Loss: 5.12, Val Loss: 0.24762, Val Reconstruction Loss: 0.21, Val KLD Loss: 0.69\r\n",
      "08/01/2025 03:55:45 Epoch: 0053, Train Loss: 0.50095, Train Reconstruction Loss: 0.22, Train KLD Loss: 5.64, Val Loss: 0.30174, Val Reconstruction Loss: 0.21, Val KLD Loss: 1.82\r\n",
      "08/01/2025 03:56:04 Epoch: 0054, Train Loss: 0.48147, Train Reconstruction Loss: 0.22, Train KLD Loss: 5.25, Val Loss: 0.28799, Val Reconstruction Loss: 0.21, Val KLD Loss: 1.53\r\n",
      "08/01/2025 03:56:24 Epoch: 0055, Train Loss: 0.49026, Train Reconstruction Loss: 0.22, Train KLD Loss: 5.42, Val Loss: 0.24365, Val Reconstruction Loss: 0.21, Val KLD Loss: 0.66\r\n",
      "08/01/2025 03:56:44 Epoch: 0056, Train Loss: 0.47234, Train Reconstruction Loss: 0.22, Train KLD Loss: 5.05, Val Loss: 0.30744, Val Reconstruction Loss: 0.21, Val KLD Loss: 1.93\r\n",
      "08/01/2025 03:57:03 Epoch: 0057, Train Loss: 0.50091, Train Reconstruction Loss: 0.22, Train KLD Loss: 5.63, Val Loss: 0.25627, Val Reconstruction Loss: 0.21, Val KLD Loss: 0.89\r\n",
      "08/01/2025 03:57:23 Epoch: 0058, Train Loss: 0.45608, Train Reconstruction Loss: 0.22, Train KLD Loss: 4.74, Val Loss: 0.28327, Val Reconstruction Loss: 0.21, Val KLD Loss: 1.46\r\n",
      "08/01/2025 03:57:43 Epoch: 0059, Train Loss: 0.48047, Train Reconstruction Loss: 0.22, Train KLD Loss: 5.25, Val Loss: 0.25977, Val Reconstruction Loss: 0.21, Val KLD Loss: 0.92\r\n",
      "08/01/2025 03:58:03 Epoch: 0060, Train Loss: 0.48446, Train Reconstruction Loss: 0.22, Train KLD Loss: 5.31, Val Loss: 0.26204, Val Reconstruction Loss: 0.21, Val KLD Loss: 0.99\r\n",
      "08/01/2025 03:58:23 Epoch: 0061, Train Loss: 0.47173, Train Reconstruction Loss: 0.22, Train KLD Loss: 5.06, Val Loss: 0.26299, Val Reconstruction Loss: 0.21, Val KLD Loss: 0.99\r\n",
      "08/01/2025 03:58:42 Epoch: 0062, Train Loss: 0.48453, Train Reconstruction Loss: 0.22, Train KLD Loss: 5.30, Val Loss: 0.25969, Val Reconstruction Loss: 0.21, Val KLD Loss: 0.95\r\n",
      "08/01/2025 03:59:01 Epoch: 0063, Train Loss: 0.45681, Train Reconstruction Loss: 0.22, Train KLD Loss: 4.74, Val Loss: 0.24798, Val Reconstruction Loss: 0.21, Val KLD Loss: 0.73\r\n",
      "08/01/2025 03:59:21 Epoch: 0064, Train Loss: 0.49171, Train Reconstruction Loss: 0.22, Train KLD Loss: 5.44, Val Loss: 0.25821, Val Reconstruction Loss: 0.21, Val KLD Loss: 0.93\r\n",
      "08/01/2025 03:59:40 Epoch: 0065, Train Loss: 0.45447, Train Reconstruction Loss: 0.22, Train KLD Loss: 4.73, Val Loss: 0.25807, Val Reconstruction Loss: 0.21, Val KLD Loss: 0.87\r\n",
      "08/01/2025 03:59:59 Epoch: 0066, Train Loss: 0.46359, Train Reconstruction Loss: 0.22, Train KLD Loss: 4.87, Val Loss: 0.24168, Val Reconstruction Loss: 0.21, Val KLD Loss: 0.57\r\n",
      "08/01/2025 04:00:19 Epoch: 0067, Train Loss: 0.42905, Train Reconstruction Loss: 0.22, Train KLD Loss: 4.22, Val Loss: 0.28151, Val Reconstruction Loss: 0.21, Val KLD Loss: 1.34\r\n",
      "08/01/2025 04:00:39 Epoch: 0068, Train Loss: 0.43386, Train Reconstruction Loss: 0.22, Train KLD Loss: 4.30, Val Loss: 0.24997, Val Reconstruction Loss: 0.21, Val KLD Loss: 0.71\r\n",
      "08/01/2025 04:00:59 Epoch: 0069, Train Loss: 0.44279, Train Reconstruction Loss: 0.22, Train KLD Loss: 4.47, Val Loss: 0.26414, Val Reconstruction Loss: 0.21, Val KLD Loss: 1.01\r\n",
      "08/01/2025 04:01:19 Epoch: 0070, Train Loss: 0.43423, Train Reconstruction Loss: 0.22, Train KLD Loss: 4.32, Val Loss: 0.24940, Val Reconstruction Loss: 0.21, Val KLD Loss: 0.70\r\n",
      "08/01/2025 04:01:39 Epoch: 0071, Train Loss: 0.41831, Train Reconstruction Loss: 0.22, Train KLD Loss: 3.95, Val Loss: 0.24150, Val Reconstruction Loss: 0.22, Val KLD Loss: 0.50\r\n",
      "08/01/2025 04:01:59 Epoch: 0072, Train Loss: 0.41416, Train Reconstruction Loss: 0.22, Train KLD Loss: 3.89, Val Loss: 0.25126, Val Reconstruction Loss: 0.21, Val KLD Loss: 0.76\r\n",
      "08/01/2025 04:02:18 Epoch: 0073, Train Loss: 0.43339, Train Reconstruction Loss: 0.22, Train KLD Loss: 4.29, Val Loss: 0.28414, Val Reconstruction Loss: 0.21, Val KLD Loss: 1.42\r\n",
      "08/01/2025 04:02:37 Epoch: 0074, Train Loss: 0.45416, Train Reconstruction Loss: 0.22, Train KLD Loss: 4.74, Val Loss: 0.25372, Val Reconstruction Loss: 0.22, Val KLD Loss: 0.76\r\n",
      "08/01/2025 04:02:57 Epoch: 0075, Train Loss: 0.40127, Train Reconstruction Loss: 0.22, Train KLD Loss: 3.62, Val Loss: 0.24668, Val Reconstruction Loss: 0.21, Val KLD Loss: 0.68\r\n",
      "08/01/2025 04:03:17 Epoch: 0076, Train Loss: 0.40804, Train Reconstruction Loss: 0.22, Train KLD Loss: 3.77, Val Loss: 0.24835, Val Reconstruction Loss: 0.21, Val KLD Loss: 0.71\r\n",
      "08/01/2025 04:03:36 Epoch: 0077, Train Loss: 0.40899, Train Reconstruction Loss: 0.22, Train KLD Loss: 3.80, Val Loss: 0.25294, Val Reconstruction Loss: 0.21, Val KLD Loss: 0.79\r\n",
      "08/01/2025 04:03:56 Epoch: 0078, Train Loss: 0.41271, Train Reconstruction Loss: 0.22, Train KLD Loss: 3.89, Val Loss: 0.33289, Val Reconstruction Loss: 0.21, Val KLD Loss: 2.37\r\n",
      "08/01/2025 04:04:16 Epoch: 0079, Train Loss: 0.48899, Train Reconstruction Loss: 0.22, Train KLD Loss: 5.40, Val Loss: 0.43120, Val Reconstruction Loss: 0.21, Val KLD Loss: 4.37\r\n",
      "08/01/2025 04:04:35 Epoch: 0080, Train Loss: 0.48620, Train Reconstruction Loss: 0.22, Train KLD Loss: 5.36, Val Loss: 0.33155, Val Reconstruction Loss: 0.22, Val KLD Loss: 2.32\r\n",
      "08/01/2025 04:04:55 Epoch: 0081, Train Loss: 0.42225, Train Reconstruction Loss: 0.22, Train KLD Loss: 4.03, Val Loss: 0.26068, Val Reconstruction Loss: 0.21, Val KLD Loss: 0.97\r\n",
      "08/01/2025 04:05:15 Epoch: 0082, Train Loss: 0.38322, Train Reconstruction Loss: 0.22, Train KLD Loss: 3.30, Val Loss: 0.23634, Val Reconstruction Loss: 0.21, Val KLD Loss: 0.43\r\n",
      "08/01/2025 04:05:35 Epoch: 0083, Train Loss: 0.35669, Train Reconstruction Loss: 0.22, Train KLD Loss: 2.77, Val Loss: 0.23769, Val Reconstruction Loss: 0.21, Val KLD Loss: 0.46\r\n",
      "08/01/2025 04:05:55 Epoch: 0084, Train Loss: 0.36110, Train Reconstruction Loss: 0.22, Train KLD Loss: 2.84, Val Loss: 0.23742, Val Reconstruction Loss: 0.21, Val KLD Loss: 0.49\r\n",
      "08/01/2025 04:06:14 Epoch: 0085, Train Loss: 0.36481, Train Reconstruction Loss: 0.22, Train KLD Loss: 2.91, Val Loss: 0.22728, Val Reconstruction Loss: 0.21, Val KLD Loss: 0.27\r\n",
      "08/01/2025 04:06:34 Epoch: 0086, Train Loss: 0.35405, Train Reconstruction Loss: 0.22, Train KLD Loss: 2.69, Val Loss: 0.23160, Val Reconstruction Loss: 0.21, Val KLD Loss: 0.36\r\n",
      "08/01/2025 04:06:53 Epoch: 0087, Train Loss: 0.34915, Train Reconstruction Loss: 0.22, Train KLD Loss: 2.61, Val Loss: 0.24445, Val Reconstruction Loss: 0.22, Val KLD Loss: 0.58\r\n",
      "08/01/2025 04:07:13 Epoch: 0088, Train Loss: 0.36340, Train Reconstruction Loss: 0.22, Train KLD Loss: 2.87, Val Loss: 0.32575, Val Reconstruction Loss: 0.21, Val KLD Loss: 2.24\r\n",
      "08/01/2025 04:07:33 Epoch: 0089, Train Loss: 0.41550, Train Reconstruction Loss: 0.22, Train KLD Loss: 3.94, Val Loss: 0.26835, Val Reconstruction Loss: 0.22, Val KLD Loss: 1.05\r\n",
      "08/01/2025 04:07:53 Epoch: 0090, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:08:13 Epoch: 0091, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:08:32 Epoch: 0092, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:08:51 Epoch: 0093, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:09:11 Epoch: 0094, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:09:30 Epoch: 0095, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:09:50 Epoch: 0096, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:10:09 Epoch: 0097, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:10:29 Epoch: 0098, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:10:49 Epoch: 0099, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:11:08 Epoch: 0100, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:11:26 Epoch: 0101, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:11:45 Epoch: 0102, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:12:05 Epoch: 0103, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:12:26 Epoch: 0104, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:12:45 Epoch: 0105, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:13:05 Epoch: 0106, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:13:23 Epoch: 0107, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:13:43 Epoch: 0108, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:14:02 Epoch: 0109, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:14:22 Epoch: 0110, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:14:41 Epoch: 0111, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:15:00 Epoch: 0112, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:15:20 Epoch: 0113, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:15:38 Epoch: 0114, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:15:58 Epoch: 0115, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:16:18 Epoch: 0116, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:16:37 Epoch: 0117, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:16:57 Epoch: 0118, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:17:16 Epoch: 0119, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:17:36 Epoch: 0120, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:17:55 Epoch: 0121, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:18:15 Epoch: 0122, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:18:35 Epoch: 0123, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:18:54 Epoch: 0124, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:19:14 Epoch: 0125, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:19:34 Epoch: 0126, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:19:53 Epoch: 0127, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:20:13 Epoch: 0128, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:20:32 Epoch: 0129, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:20:52 Epoch: 0130, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:21:11 Epoch: 0131, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:21:29 Epoch: 0132, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:21:48 Epoch: 0133, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:22:07 Epoch: 0134, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:22:26 Epoch: 0135, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:22:46 Epoch: 0136, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:23:05 Epoch: 0137, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:23:24 Epoch: 0138, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:23:42 Epoch: 0139, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:24:02 Epoch: 0140, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:24:22 Epoch: 0141, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:24:41 Epoch: 0142, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:25:00 Epoch: 0143, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:25:20 Epoch: 0144, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:25:40 Epoch: 0145, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:25:58 Epoch: 0146, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:26:17 Epoch: 0147, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:26:37 Epoch: 0148, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:26:51 Epoch: 0149, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:27:11 Epoch: 0150, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:27:30 Epoch: 0151, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:27:50 Epoch: 0152, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:28:10 Epoch: 0153, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:28:30 Epoch: 0154, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:28:49 Epoch: 0155, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:29:09 Epoch: 0156, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:29:29 Epoch: 0157, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:29:49 Epoch: 0158, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:30:08 Epoch: 0159, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:30:28 Epoch: 0160, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:30:46 Epoch: 0161, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:31:04 Epoch: 0162, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:31:24 Epoch: 0163, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:31:44 Epoch: 0164, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:32:03 Epoch: 0165, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:32:23 Epoch: 0166, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:32:43 Epoch: 0167, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:33:03 Epoch: 0168, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:33:22 Epoch: 0169, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:33:41 Epoch: 0170, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:34:00 Epoch: 0171, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:34:20 Epoch: 0172, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:34:40 Epoch: 0173, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:34:59 Epoch: 0174, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:35:19 Epoch: 0175, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:35:38 Epoch: 0176, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:35:58 Epoch: 0177, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:36:18 Epoch: 0178, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:36:38 Epoch: 0179, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:36:58 Epoch: 0180, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:37:17 Epoch: 0181, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:37:37 Epoch: 0182, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:37:56 Epoch: 0183, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:38:16 Epoch: 0184, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:38:35 Epoch: 0185, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:38:54 Epoch: 0186, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:39:14 Epoch: 0187, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:39:34 Epoch: 0188, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:39:53 Epoch: 0189, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:40:13 Epoch: 0190, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:40:32 Epoch: 0191, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:40:52 Epoch: 0192, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:41:12 Epoch: 0193, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:41:32 Epoch: 0194, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:41:52 Epoch: 0195, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:42:11 Epoch: 0196, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:42:30 Epoch: 0197, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:42:50 Epoch: 0198, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:43:10 Epoch: 0199, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:43:29 Epoch: 0200, Train Loss: nan, Train Reconstruction Loss: nan, Train KLD Loss: nan, Val Loss: nan, Val Reconstruction Loss: nan, Val KLD Loss: nan\r\n",
      "08/01/2025 04:45:00 Epoch: 0005, Train Loss: nan, Val Loss: nan\r\n",
      "08/01/2025 04:46:30 Epoch: 0010, Train Loss: nan, Val Loss: nan\r\n",
      "08/01/2025 04:48:01 Epoch: 0015, Train Loss: nan, Val Loss: nan\r\n",
      "08/01/2025 04:49:30 Epoch: 0020, Train Loss: nan, Val Loss: nan\r\n",
      "08/01/2025 04:51:00 Epoch: 0025, Train Loss: nan, Val Loss: nan\r\n",
      "08/01/2025 04:52:29 Epoch: 0030, Train Loss: nan, Val Loss: nan\r\n",
      "08/01/2025 04:53:58 Epoch: 0035, Train Loss: nan, Val Loss: nan\r\n",
      "08/01/2025 04:55:28 Epoch: 0040, Train Loss: nan, Val Loss: nan\r\n",
      "08/01/2025 04:56:59 Epoch: 0045, Train Loss: nan, Val Loss: nan\r\n",
      "08/01/2025 04:58:29 Epoch: 0050, Train Loss: nan, Val Loss: nan\r\n",
      "08/01/2025 04:59:58 Epoch: 0055, Train Loss: nan, Val Loss: nan\r\n",
      "08/01/2025 05:01:30 Epoch: 0060, Train Loss: nan, Val Loss: nan\r\n",
      "08/01/2025 05:03:01 Epoch: 0065, Train Loss: nan, Val Loss: nan\r\n",
      "08/01/2025 05:04:32 Epoch: 0070, Train Loss: nan, Val Loss: nan\r\n",
      "08/01/2025 05:06:01 Epoch: 0075, Train Loss: nan, Val Loss: nan\r\n",
      "08/01/2025 05:07:31 Epoch: 0080, Train Loss: nan, Val Loss: nan\r\n",
      "08/01/2025 05:09:02 Epoch: 0085, Train Loss: nan, Val Loss: nan\r\n",
      "08/01/2025 05:10:33 Epoch: 0090, Train Loss: nan, Val Loss: nan\r\n",
      "08/01/2025 05:12:03 Epoch: 0095, Train Loss: nan, Val Loss: nan\r\n",
      "08/01/2025 05:13:33 Epoch: 0100, Train Loss: nan, Val Loss: nan\r\n",
      "Processing test set: 100%|████████████████████████| 4/4 [00:20<00:00,  5.05s/it]\r\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "!python code/main.py --lr 0.01 --hidden-dim-encoder 256 --hidden-dim-decoder 128 --latent-dim 32 --n-layers-encoder 3 --n-layers-decoder 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a3f21e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T05:13:55.280683Z",
     "iopub.status.busy": "2025-01-08T05:13:55.280301Z",
     "iopub.status.idle": "2025-01-08T05:13:55.406126Z",
     "shell.execute_reply": "2025-01-08T05:13:55.404952Z"
    },
    "papermill": {
     "duration": 0.177574,
     "end_time": "2025-01-08T05:13:55.408347",
     "exception": false,
     "start_time": "2025-01-08T05:13:55.230773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoencoder.pth.tar  code  data  output.csv  README.md\trequirements.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 27253.548022,
   "end_time": "2025-01-08T05:13:56.011746",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-07T21:39:42.463724",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
